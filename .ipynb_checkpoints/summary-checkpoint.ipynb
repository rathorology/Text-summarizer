{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"capterra/capterra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1192123, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productName</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>productId</th>\n",
       "      <th>title</th>\n",
       "      <th>consText</th>\n",
       "      <th>prosText</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EchoSpan 360 Degree Feedback</td>\n",
       "      <td>1584391</td>\n",
       "      <td>{'$numberInt': '106417'}</td>\n",
       "      <td>Elegant simplicity</td>\n",
       "      <td>Honestly, I can't think of anything.  The reports are straightforward and easy to understand.  Some of the competitors have elegant data visualization dashboards but charge 2x the money for it.</td>\n",
       "      <td>Many software packages are \"over built\" for the functionality businesses actually need.  EchoSpan is one of the few companies who designs software that walks that line perfectly.  The basics are c...</td>\n",
       "      <td>I am a fan!  EchoSpan has proven to be an ideal partner for our engagement surveys.  The staff are highly responsive and the communication on projects is unsurpassed.  These characteristics are no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EchoSpan 360 Degree Feedback</td>\n",
       "      <td>138744</td>\n",
       "      <td>{'$numberInt': '106417'}</td>\n",
       "      <td>The Echospan survey in a team effectiveness application.</td>\n",
       "      <td>There are a few limitations especially in formatting of final reports. But, for the flexibility and at the cost I can't complain.</td>\n",
       "      <td>Ours is a special application of the usual 360 degree tool. We aren't using it as a competency survey for individuals. Instead, we have developed a team effectiveness assessment that runs on a 360...</td>\n",
       "      <td>I am a confirmed fan. I searched for months for a 360 survey solution that was capable, flexible, and affordable. I found a lot of capability and  flexibility out there. Affordability was another ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EchoSpan 360 Degree Feedback</td>\n",
       "      <td>125934</td>\n",
       "      <td>{'$numberInt': '106417'}</td>\n",
       "      <td>Overall, a good experience!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have just recently started using the system and overall, I like it! It has many more features then the old system I used to use. I like that you can upload Targets and Raters into the system wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EchoSpan 360 Degree Feedback</td>\n",
       "      <td>137048</td>\n",
       "      <td>{'$numberInt': '106417'}</td>\n",
       "      <td>A great product backed by a solid team</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I can't say enough good things about the Echospan 360-degree feedback tool! The platform is flexible, intuitive, and easy-to-use. We have been able to configure a variety of custom assessments ali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EchoSpan 360 Degree Feedback</td>\n",
       "      <td>416484</td>\n",
       "      <td>{'$numberInt': '106417'}</td>\n",
       "      <td>The tool was very easy to use and utilizes great reports for displaying results.</td>\n",
       "      <td>The limitation on number of raters when it comes to larger projects. It would be helpful to allow for more than 100 raters without costing per rater and having to add raters through contacting sup...</td>\n",
       "      <td>The ease of use. I have had very few questions on how to login and use the software from raters. I like the competency library and the reading suggestions.</td>\n",
       "      <td>Very targeted results helped to identify which competencies were in need of improvement as well as what areas the employee has excelled at.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    productName  reviewId                 productId  \\\n",
       "0  EchoSpan 360 Degree Feedback   1584391  {'$numberInt': '106417'}   \n",
       "1  EchoSpan 360 Degree Feedback    138744  {'$numberInt': '106417'}   \n",
       "2  EchoSpan 360 Degree Feedback    125934  {'$numberInt': '106417'}   \n",
       "3  EchoSpan 360 Degree Feedback    137048  {'$numberInt': '106417'}   \n",
       "4  EchoSpan 360 Degree Feedback    416484  {'$numberInt': '106417'}   \n",
       "\n",
       "                                                                               title  \\\n",
       "0                                                                 Elegant simplicity   \n",
       "1                           The Echospan survey in a team effectiveness application.   \n",
       "2                                                        Overall, a good experience!   \n",
       "3                                             A great product backed by a solid team   \n",
       "4  The tool was very easy to use and utilizes great reports for displaying results.    \n",
       "\n",
       "                                                                                                                                                                                                  consText  \\\n",
       "0        Honestly, I can't think of anything.  The reports are straightforward and easy to understand.  Some of the competitors have elegant data visualization dashboards but charge 2x the money for it.   \n",
       "1                                                                       There are a few limitations especially in formatting of final reports. But, for the flexibility and at the cost I can't complain.    \n",
       "2                                                                                                                                                                                                      NaN   \n",
       "3                                                                                                                                                                                                      NaN   \n",
       "4  The limitation on number of raters when it comes to larger projects. It would be helpful to allow for more than 100 raters without costing per rater and having to add raters through contacting sup...   \n",
       "\n",
       "                                                                                                                                                                                                  prosText  \\\n",
       "0  Many software packages are \"over built\" for the functionality businesses actually need.  EchoSpan is one of the few companies who designs software that walks that line perfectly.  The basics are c...   \n",
       "1  Ours is a special application of the usual 360 degree tool. We aren't using it as a competency survey for individuals. Instead, we have developed a team effectiveness assessment that runs on a 360...   \n",
       "2                                                                                                                                                                                                      NaN   \n",
       "3                                                                                                                                                                                                      NaN   \n",
       "4                                              The ease of use. I have had very few questions on how to login and use the software from raters. I like the competency library and the reading suggestions.   \n",
       "\n",
       "                                                                                                                                                                                                  comments  \n",
       "0  I am a fan!  EchoSpan has proven to be an ideal partner for our engagement surveys.  The staff are highly responsive and the communication on projects is unsurpassed.  These characteristics are no...  \n",
       "1  I am a confirmed fan. I searched for months for a 360 survey solution that was capable, flexible, and affordable. I found a lot of capability and  flexibility out there. Affordability was another ...  \n",
       "2  I have just recently started using the system and overall, I like it! It has many more features then the old system I used to use. I like that you can upload Targets and Raters into the system wit...  \n",
       "3  I can't say enough good things about the Echospan 360-degree feedback tool! The platform is flexible, intuitive, and easy-to-use. We have been able to configure a variety of custom assessments ali...  \n",
       "4                                                              Very targeted results helped to identify which competencies were in need of improvement as well as what areas the employee has excelled at.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_sent_words(sent):\n",
    "#     sent = str(sent).lower()\n",
    "#     sent = RegexpTokenizer(r'\\w+').tokenize(sent)\n",
    "#     stop = set(stopwords.words('english'))\n",
    "#     sent_words = [word for word in sent if word not in stop]\n",
    "\n",
    "#     return sent_words\n",
    "# filtered = clean_sent_words(df.comments)\n",
    "# filtered = list()\n",
    "# for sent in df.comments.tolist():\n",
    "#     filtered.append(clean_sent_words(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productName         0\n",
      "reviewId            0\n",
      "productId           0\n",
      "title           43197\n",
      "consText       158000\n",
      "prosText       131431\n",
      "comments       487433\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.isnull().sum())\n",
    "rew = df[['title','comments']]\n",
    "rew.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"html.parser\").text\n",
    "\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text = []\n",
    "for t in rew['comments']:\n",
    "    cleaned_text.append(text_cleaner(t))\n",
    "def summary_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString    \n",
    "#Call the above function\n",
    "cleaned_summary = []\n",
    "for t in rew['title']:\n",
    "    cleaned_summary.append(summary_cleaner(t))\n",
    "\n",
    "rew['comments']=cleaned_text\n",
    "rew['title']=cleaned_summary\n",
    "rew['title'].replace('', np.nan, inplace=True)\n",
    "rew.dropna(axis=0,inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew['title'] = rew['title'].apply(lambda x : '_START_ '+ x + ' _END_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: fan echospan proven ideal partner engagement surveys staff highly responsive communication projects unsurpassed characteristics easy find many bigger technology companies bigger always better echospan smart choice solid software functionality star customer service\n",
      "Summary: _START_ elegant simplicity  _END_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"Review:\",rew['comments'][i])\n",
    "    print(\"Summary:\",rew['title'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in rew['comments']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in rew['title']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text=80 \n",
    "max_len_summary=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(rew['comments'],rew['title'],test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215715                                                                                                                      overall experience bluebeam satisfactory bluebeam makes dealing pdfs field ton easier\n",
       "418129    company used navotar long intend stay long time customers ease use software amazing especially quick customer service even joined got back interest email within business day website built great to...\n",
       "519684                                                                                                            great tool always always laptop computer mobile version helpful stay connected office personnel\n",
       "973902                                                                                                                                     help hard get even easy questions seems person get tell area knowledge\n",
       "33755                                                                            qbo consistently becoming robust true intuit fashion listening users adding new features constantly qbo app sync seamlessly file\n",
       "                                                                                                           ...                                                                                                   \n",
       "625838    positive experience dealing novatek staff knowledgeable professionals make concerted effort satisfy customer needs department impressed way handled software installation validations technical staf...\n",
       "259800                          great asset board busy business people community needed able everything centralized location electronic especially voting signatures helped conduct business faster communication\n",
       "204719    use aha manage entire product portfolio aha tried manage feature backlogs spreadsheets multiple product owners nightmare build maintain consolidated roadmap much less track cross product feature d...\n",
       "751016    rep nothing helpful starting initial conversations staff members consistent support email questions always kind quick respond questions expensepoint worked well financial software happy manually e...\n",
       "532407    kaspersky endpoint security provides security broad range threats malicious programs although deal sensitive information suffered data security breach cyber attach since started using slow perform...\n",
       "Name: comments, Length: 631108, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size   =  len(x_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing a tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
    "y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_voc_size  =   len(y_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 80)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 80, 500)      38908500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 80, 500), (N 2002000     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 80, 500), (N 2002000     lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 500)    23060000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 80, 500), (N 2002000     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 500),  2002000     embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 500),  500500      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 1000)   0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 46120)  46166120    concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 116,643,120\n",
      "Trainable params: 116,643,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K \n",
    "K.clear_session() \n",
    "latent_dim = 500 \n",
    "\n",
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_len_text,)) \n",
    "enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n",
    "\n",
    "#LSTM 1 \n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# Set up the decoder. \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (631108, 9, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-93aee35d4b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/great_learning/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/PycharmProjects/great_learning/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/great_learning/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/great_learning/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/great_learning/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/PycharmProjects/great_learning/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2489\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m       sample_weights, _, _ = training_utils.handle_partial_sample_weights(\n",
      "\u001b[0;32m~/PycharmProjects/great_learning/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    782\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[1;32m    783\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m                          \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m                          \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                          \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (631108, 9, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=512, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6462522140850906102\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16177132383971998881\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7476007345341098653\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1324744704\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6111753589081393272\n",
      "physical_device_desc: \"device: 0, name: GeForce MX250, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce MX250, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
